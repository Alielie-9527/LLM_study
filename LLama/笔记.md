# 一、混合精度（Mixed Precision Training)

## 1.混合精度训练的基本概念

​	传统深度学习训练中， 模型权重和计算通常使用**Fp32（32位浮点数）**。这种方式在精度上没有问题，但是 **计算量大**， **内存占用高（显存）**。

​	混合精度训练，则是根据训练过程中的不同计算部分，使用不同精度

（1）**单精度浮点训练**可以带来以下好处：

1. 减少对 GPU 显存的需求，或者在 GPU 显存保持不变的情况下，可以支持更大模型和更大的 batch size；

2. 降低显存读写的带宽压力；

3. 加速 GPU 数学运算速度 (需要 GPU 支持)；按照 NVIDA 数据，GPU 上`FP16`计算吞吐量是`FP32`的 2~8 倍。

   ![image-20251208231956008](C:\Users\12472\AppData\Roaming\Typora\typora-user-images\image-20251208231956008.png)

（2）**三种技术**：



1. **FP32的主备份：**

   ​	在[混合精度训练](https://zhida.zhihu.com/search?content_id=163310307&content_type=Article&match_order=1&q=混合精度训练&zhida_source=entity)中，权重、激活、梯度都采用的是半精度存储的。为了匹配单精度网络的精度，在优化的整个过程中，需要拷贝一份**单精度模型FP32**作为主备份，而**训练过程中使用的是FP16**进行计算。

​		深度学习模型在训练后期，梯度值通常非常小（例如1e-7量级），乘以学习率后更新量可能极小（1e-10量级）。在FP16精度下，这种极小的更新量在加法操作中可能被大数"吞没"（Swallowing），导致参数无法有效更新。

​	**解决方案**是维护两份权重副本：

- **FP16副本**：用于前向和反向传播，享受计算加速
- **FP32副本（Master Weight）**：用于参数更新，保证更新精度



- **流程：**模型在显存里存有一份 **FP32 的权重副本（Master Weights）**。
  - Forward 开始时，将 FP32 权重**转换**为 FP16 权重（用于计算）。
  - Forward 和 Backward 都在 FP16 下进行（利用 Tensor Cores 加速），算出 **FP16 的梯度**。
  - Update 阶段，将 FP16 的梯度转换回 FP32，并**更新到 FP32 的 Master Weights 上**。





2.**梯度缩放（Loss Scaling）**

​	由于Fp16的范围较小，在训练后期，计算出来的梯度可能非常小，导致被舍入为0，造成**梯度下溢**的问题（变为0）。

处理方式：

- 计算梯度之前，放大损失（实际上就是放大梯度）

- 更新的时候恢复真实的梯度即可

  - **解决方案：**：

    - **Scale Up:** 在反向传播开始前，将 Loss 乘以一个巨大的系数 `SS`（比如 `216=65536216=65536`）。
    - **Backward:** 根据链式法则，所有的梯度都会被放大 `SS` 倍。这样原本极小的梯度（比如 `10−710−7`）就变成了较大的值（`0.0060.006`），安全地落在了 FP16 的表示范围内。
    - **Unscale:** 在更新参数之前，把梯度除以 `SS`，还原回真实大小。

    - **动态调整：** 框架通常使用**动态 Loss Scaling**。如果检测到梯度溢出（出现了 Inf/NaN），就减小 `SS`；如果连续很多步没溢出，就尝试增大 `SS`。

      



3.**自动精度切换**

​	在混合精度训练中，使用的是**自动混合精度**（Automatic Mixed Precision, AMP），即深度学习框架（如 PyTorch）会自动选择在哪些操作中使用 FP16，在哪些操作中使用 FP32。

​	**精度累加技术**主要用于解决计算过程中的舍入误差累积问题。在矩阵乘法等操作中，尽管输入和输出使用FP16，但中间累加过程使用FP32进行，最终结果再转换回FP16存储。

​	以矩阵乘法为例，当计算大矩阵的点积时，多个小数的乘积相加可能产生累积误差。使用FP16进行累加时，由于精度有限，这些误差可能导致结果不准确。而使用FP32进行中间累加，然后将结果舍入到FP16，可以大幅减少这种误差。



## 2.详细机制：（参考google AI studio)

### 核心决策机制：黑白名单 (The Casting Policies)

框架（如 PyTorch）维护了三个核心列表，用来决定某个算子（Operator）在 Forward 阶段应该用 FP16 还是 FP32。

#### 1. 白名单 (Allowlist) -> 强制 FP16

这些算子计算量大，且对精度不敏感（具有较强的容错性）。使用 FP16 可以利用 NVIDIA GPU 的 **Tensor Cores** 进行数倍加速。

- **典型算子：** 
  - **矩阵乘法：** Linear (MatMul, Gemm)
  - **卷积：** Conv1d, Conv2d, Conv3d
  - **循环神经网络：** RNN, LSTM, GRU
- **理由：** 这些是深度学习中的“计算密集型”瓶颈，且在 FP16 下通常表现良好。

#### 2. 黑名单 (Denylist) -> 强制 FP32

这些算子对精度极度敏感，或者涉及数值范围很大的计算。如果用 FP16，极易发生**溢出（Overflow，变无穷大）或下溢（Underflow，变0）**。

- **典型算子：** 
  - **指数运算**： exp (容易溢出)
  - **求和/归一化：** sum, softmax (求和累积误差大), LayerNorm, GroupNorm
  - **损失函数：** CrossEntropyLoss, L1Loss, MSELoss
  - **大数值计算：** pow, log
- **理由：** FP16 的最大值只有 65504。做 exp 或大范围 sum 时很容易超过这个值，导致训练 NaN（Not a Number）。

#### 3. 灰名单 (Inferlist / Promote) -> 随波逐流

这些算子通常是逐元素操作（Element-wise），计算量小（受限于内存带宽而非算力）。它们使用什么精度，取决于**输入数据的精度**。

- **典型算子：****激活函数：** ReLU, Sigmoid, Tanh**基础运算：** Add, Sub, Mul
- **策略：** 如果输入是 FP16，就跑 FP16；如果输入是 FP32，就跑 FP32。这避免了不必要的来回转换（Cast）开销。

```pyhon
		ctx =  torch.amp.autocast(device_type=args.device.split(':')[0], dtype=getattr(torch, args.dtype))
         
    	scaler = torch.amp.GradScaler('cuda', enabled=(args.dtype == 'float16'))
        # ==========开启自动混合精度=============
        with ctx:
            # 前向传播
            out = model(X, Y)
            # 计算损失并除以累积步数（用于梯度累积）
            loss = out.last_loss / args.accumulation_steps
            # 将loss_mask展平为一维
            loss_mask = loss_mask.view(-1)
            # 应用掩码计算有效损失（忽略padding位置）
            # 优化3: 添加 1e-6 防止除以零的数值稳定性问题
            loss = torch.sum(loss * loss_mask) / (loss_mask.sum() + 1e-6)

   
        # 反向传播
        # 使用 scaler 处理梯度缩放，防止 FP16 下溢
        # 梯度放大，并 进行反向传播
        scaler.scale(loss).backward()

        # 梯度累积：每 accumulation_steps 步更新一次参数
        if (batch_idx + 1) % args.accumulation_steps == 0:
            # 梯度裁剪前必须先 unscale 梯度 梯度缩小
            scaler.unscale_(optimizer)
            
            # 梯度裁剪，防止梯度爆炸
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
            
            # 更新参数
            scaler.step(optimizer)
            
            # 更新 scaler 的缩放因子
            scaler.update()
            
            # 清空梯度       
            # 优化4: set_to_none=True 比默认的 set to 0 更快，减少内存写操作
            optimizer.zero_grad(set_to_none=True)
```



## 3.举例

步骤      操作类型       输入数据类型      权重数据类型(在模型里)      实际计算精度      输出数据类型

Linear    [白名单]       FP32/FP16  --->   FP32                  FP16 (临时转)     FP16
                                           (Autocast把权重转FP16)
                                               |
                                               v
Softmax   [黑名单]       FP16       --->   (无权重)               FP32 (临时转)     FP32
                                           (Autocast把输入转FP32)
                                               |
                                               v
Loss      [黑名单]       FP32       --->   (无权重)               FP32             FP32



# 二、梯度裁剪

​	梯度直观来说就是在当前这一点的斜率。我们希望最小化模型损失，实际上就是找到空间中的最低点。我们希望能坡度（梯度)平缓，迈着小碎步下山，而不是（梯度爆炸）飞速冲下山。那么梯度实际上最重要的是梯度的方向（个人观点，因为方向决定了往哪走，lr 和梯度的大小共同决定了走的步子）。

​	业界标准，按范数裁剪（Norm Clipping）， clip_grad_norm_:
$$
g_{\text{new}} = \begin{cases}
g_{\text{old}}, & \text{if } \|g_{\text{old}}\|_2 \leq \text{max\_norm} \\
g_{\text{old}} \times \dfrac{\text{max\_norm}}{\|g_{\text{old}}\|_2}, & \text{if } \|g_{\text{old}}\|_2 > \text{max\_norm}
\end{cases}
$$
