# 一、混合精度（Mixed Precision Training)

## 1.混合精度训练的基本概念

​	传统深度学习训练中， 模型权重和计算通常使用**Fp32（32位浮点数）**。这种方式在精度上没有问题，但是 **计算量大**， **内存占用高（显存）**。

​	混合精度训练，则是根据训练过程中的不同计算部分，使用不同精度

（1）**单精度浮点训练**可以带来以下好处：

1. 减少对 GPU 显存的需求，或者在 GPU 显存保持不变的情况下，可以支持更大模型和更大的 batch size；

2. 降低显存读写的带宽压力；

3. 加速 GPU 数学运算速度 (需要 GPU 支持)；按照 NVIDA 数据，GPU 上`FP16`计算吞吐量是`FP32`的 2~8 倍。



（2）**三种技术**：



1. **FP32的主备份：**

   ​	在**混合精度训练**中，权重、激活、梯度都采用的是半精度存储的。为了匹配单精度网络的精度，在优化的整个过程中，需要拷贝一份**单精度模型FP32**作为主备份，而**训练过程中使用的是FP16**进行计算。

​		深度学习模型在训练后期，梯度值通常非常小（例如1e-7量级），乘以学习率后更新量可能极小（1e-10量级）。在FP16精度下，这种极小的更新量在加法操作中可能被大数"吞没"（Swallowing），导致参数无法有效更新。

​	**解决方案**是维护两份权重副本：

- **FP16副本**：用于前向和反向传播，享受计算加速
- **FP32副本（Master Weight）**：用于参数更新，保证更新精度



- **流程**：模型在显存里存有一份 **FP32 的权重副本（Master Weights）**。
  - Forward 开始时，将 FP32 权重**转换**为 FP16 权重（用于计算）。
  - Forward 和 Backward 都在 FP16 下进行（利用 Tensor Cores 加速），算出 **FP16 的梯度**。
  - Update 阶段，将 FP16 的梯度转换回 FP32，并**更新到 FP32 的 Master Weights 上**。





2.**梯度缩放（Loss Scaling）**

​	由于Fp16的范围较小，在训练后期，计算出来的梯度可能非常小，导致被舍入为0，造成**梯度下溢**的问题（变为0）。

处理方式：

- 计算梯度之前，放大损失（实际上就是放大梯度）

- 更新的时候恢复真实的梯度即可

  - **解决方案：**：

    - **Scale Up:** 在反向传播开始前，将 Loss 乘以一个巨大的系数 `SS`（比如 `216=65536216=65536`）。
    - **Backward:** 根据链式法则，所有的梯度都会被放大 `SS` 倍。这样原本极小的梯度（比如 `10−710−7`）就变成了较大的值（`0.0060.006`），安全地落在了 FP16 的表示范围内。
    - **Unscale:** 在更新参数之前，把梯度除以 `SS`，还原回真实大小。

    - **动态调整：** 框架通常使用**动态 Loss Scaling**。如果检测到梯度溢出（出现了 Inf/NaN），就减小 `SS`；如果连续很多步没溢出，就尝试增大 `SS`。

      



3.**自动精度切换**

​	在混合精度训练中，使用的是**自动混合精度**（Automatic Mixed Precision, AMP），即深度学习框架（如 PyTorch）会自动选择在哪些操作中使用 FP16，在哪些操作中使用 FP32。

​	**精度累加技术**主要用于解决计算过程中的舍入误差累积问题。在矩阵乘法等操作中，尽管输入和输出使用FP16，但中间累加过程使用FP32进行，最终结果再转换回FP16存储。

​	以矩阵乘法为例，当计算大矩阵的点积时，多个小数的乘积相加可能产生累积误差。使用FP16进行累加时，由于精度有限，这些误差可能导致结果不准确。而使用FP32进行中间累加，然后将结果舍入到FP16，可以大幅减少这种误差。



## 2.详细机制：（参考google AI studio)

### 核心决策机制：黑白名单 (The Casting Policies)

框架（如 PyTorch）维护了三个核心列表，用来决定某个算子（Operator）在 Forward 阶段应该用 FP16 还是 FP32。

#### 1. 白名单 (Allowlist) -> 强制 FP16

这些算子计算量大，且对精度不敏感（具有较强的容错性）。使用 FP16 可以利用 NVIDIA GPU 的 **Tensor Cores** 进行数倍加速。

- **典型算子：** 
  - **矩阵乘法：** Linear (MatMul, Gemm)
  - **卷积：** Conv1d, Conv2d, Conv3d
  - **循环神经网络：** RNN, LSTM, GRU
- **理由：** 这些是深度学习中的“计算密集型”瓶颈，且在 FP16 下通常表现良好。

#### 2. 黑名单 (Denylist) -> 强制 FP32

这些算子对精度极度敏感，或者涉及数值范围很大的计算。如果用 FP16，极易发生**溢出（Overflow，变无穷大）或下溢（Underflow，变0）**。

- **典型算子：** 
  - **指数运算**： exp (容易溢出)
  - **求和/归一化：** sum, softmax (求和累积误差大), LayerNorm, GroupNorm
  - **损失函数：** CrossEntropyLoss, L1Loss, MSELoss
  - **大数值计算：** pow, log
- **理由：** FP16 的最大值只有 65504。做 exp 或大范围 sum 时很容易超过这个值，导致训练 NaN（Not a Number）。

#### 3. 灰名单 (Inferlist / Promote) -> 随波逐流

这些算子通常是逐元素操作（Element-wise），计算量小（受限于内存带宽而非算力）。它们使用什么精度，取决于**输入数据的精度**。

- **典型算子：****激活函数：** ReLU, Sigmoid, Tanh**基础运算：** Add, Sub, Mul
- **策略：** 如果输入是 FP16，就跑 FP16；如果输入是 FP32，就跑 FP32。这避免了不必要的来回转换（Cast）开销。

```pyhon
		ctx =  torch.amp.autocast(device_type=args.device.split(':')[0], dtype=getattr(torch, args.dtype))
         
    	scaler = torch.amp.GradScaler('cuda', enabled=(args.dtype == 'float16'))
        # ==========开启自动混合精度=============
        with ctx:
            # 前向传播
            out = model(X, Y)
            # 计算损失并除以累积步数（用于梯度累积）
            loss = out.last_loss / args.accumulation_steps
            # 一、混合精度（Mixed Precision Training）

            ## 1. 基本概念

            传统训练通常用 **FP32**；混合精度会在合适的算子上用 **FP16**，以降低显存、带宽占用并加速计算（在支持 Tensor Core 的 GPU 上常能获得 2~8 倍吞吐提升）。

            三项关键技术：

            1) **FP32 主副本**：权重、激活、梯度在前后向使用 FP16 计算，但维护一份 FP32 主权重用于参数更新，避免极小更新被吞没。
            2) **梯度缩放（Loss Scaling）**：反向前将 loss 乘以一个系数放大梯度，更新前再除回；多数框架提供动态缩放，检测到溢出就调小系数，稳定后再调大。
            3) **自动混合精度 (AMP)**：框架根据算子特性选择精度；矩阵乘法/卷积等计算密集型算子用 FP16，softmax/归一化/损失等易数值不稳的算子强制用 FP32；逐元素算子随输入精度走，并在累加时用 FP32 减少舍入误差。

            ## 2. 详细机制（PyTorch AMP 示例）

            ### 算子精度策略

            - **Allowlist（强制 FP16）**：MatMul、Conv、RNN 等计算密集型算子。
            - **Denylist（强制 FP32）**：softmax、LayerNorm、求和、exp/log、各类损失函数。
            - **灰名单（跟随输入）**：ReLU/Sigmoid/Tanh 及基础加减乘除等逐元素算子。

            ### 常见代码片段

            ```python
            ctx = torch.amp.autocast(device_type=args.device.split(':')[0], dtype=getattr(torch, args.dtype))
            scaler = torch.amp.GradScaler('cuda', enabled=(args.dtype == 'float16'))

            with ctx:
                out = model(X, Y)
                loss = out.last_loss / args.accumulation_steps
                loss_mask = loss_mask.view(-1)
                loss = torch.sum(loss * loss_mask) / (loss_mask.sum() + 1e-6)

            scaler.scale(loss).backward()

            if (batch_idx + 1) % args.accumulation_steps == 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad(set_to_none=True)
            ```

            ## 3. 精度切换示例

            | 步骤 | 操作类型 | 输入类型 | 权重存储 | 实际计算精度 | 输出类型 |
            | --- | --- | --- | --- | --- | --- |
            | 1 | Linear（白名单） | FP32/FP16 | FP32（运行时转 FP16） | FP16 | FP16 |
            | 2 | Softmax（黑名单） | FP16 | 无 | FP32 | FP32 |
            | 3 | Loss（黑名单） | FP32 | 无 | FP32 | FP32 |

            # 二、梯度裁剪

            梯度裁剪通常采用按范数裁剪（`clip_grad_norm_`）：
            $$
            g_{\text{new}} = \begin{cases}
            g_{\text{old}}, & \text{if } \|g_{\text{old}}\|_2 \leq \text{max\_norm} \\
            g_{\text{old}} \times \dfrac{\text{max\_norm}}{\|g_{\text{old}}\|_2}, & \text{if } \|g_{\text{old}}\|_2 > \text{max\_norm}
            \end{cases}
            $$

            核心目的是限制更新步长，避免梯度爆炸，同时保持更新方向不变。



