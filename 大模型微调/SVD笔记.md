# SVD笔记

---

### 第一部分：SVD 到底是在干什么？（直观理解）

想象你有一个矩阵 $A$。在线性代数中，**矩阵就是一个变换（Transformation）**。
当你把矩阵 $A$ 乘以一个向量 $x$（即 $Ax$）时，就是在对 $x$ 进行拉伸、旋转或翻转。

SVD 的核心思想是：**任何一个复杂的矩阵变换，都可以拆解为三个简单的动作。**

公式长这样：
$$ A = U \Sigma V^T $$

我们可以把这三个矩阵看作三个连续的动作，作用在一个圆上：

1.  **$V^T$（旋转）**：先把坐标系旋转一下。
2.  **$\Sigma$（拉伸）**：沿着坐标轴进行拉伸或压缩（圆变成了椭圆）。
3.  **$U$（再旋转）**：最后把这个椭圆再旋转一次。

**总结：** SVD 告诉我们，不管矩阵 $A$ 把空间扭曲成什么奇怪的样子，本质上都只是“**旋转 $\rightarrow$ 拉伸 $\rightarrow$ 旋转**”。

---

### 第二部分：必须掌握的“前置知识”（基础补课）

在推导 SVD 之前，需要了解以下 3 个关键的线性代数概念。

#### 1. 正交矩阵 (Orthogonal Matrix)
*   **定义**：如果一个方阵 $Q$ 的列向量两两垂直（正交）且长度为 1（单位化），它就是正交矩阵。
*   **性质**：**$Q^T Q = I$**（转置等于逆，即 $Q^T = Q^{-1}$）。
*   **物理意义**：正交矩阵代表**纯旋转**（或者翻转），它不会改变向量的长度，只改变方向。
*   *SVD 中的 $U$ 和 $V$ 都是正交矩阵。*

#### 2. 特征值分解 (Eigendecomposition)
*   **公式**：$A x = \lambda x$。
*   **直观**：有些向量 $x$ 在经过矩阵 $A$ 变换后，**方向不改变**，只是长度伸缩了。这个伸缩的倍数 $\lambda$ 叫特征值，向量 $x$ 叫特征向量。
*   **局限**：只有**方阵**（行数=列数）才能做特征值分解。而 SVD 能处理任意形状的矩阵。

#### 3. 对称矩阵 (Symmetric Matrix)
*   **定义**：$A = A^T$（沿对角线翻折后一模一样）。
*   **超级重要的性质（谱定理）**：对称矩阵一定可以被分解为 $A = Q \Lambda Q^T$。
    *   它的特征向量是**相互垂直（正交）**的。
    *   它的特征值都是**实数**。
*   *SVD 的推导完全依赖这个性质。*

---

### 第三部分：SVD 的数学定义

对于任意一个 $m \times n$ 的矩阵 $A$，我们都能找到分解：

$$ A = U \Sigma V^T $$

其中：
1.  **$U$** ($m \times m$)：**左奇异向量**。它是一个正交矩阵。
2.  **$\Sigma$** ($m \times n$)：**奇异值矩阵**。除了对角线上的元素外，其余全是 0。对角线上的元素 $\sigma_i$ 称为**奇异值**，且通常从大到小排列 ($\sigma_1 \ge \sigma_2 \dots \ge 0$)。这些值代表了“拉伸”的程度。
3.  **$V^T$** ($n \times n$)：**右奇异向量**的转置。$V$ 也是一个正交矩阵。

---

### 第四部分：SVD 是怎么推导出来的？（核心逻辑）

**目标**：我们想求出 $U, \Sigma, V$。
**困境**：$A$ 可能不是方阵，没法直接求特征值。
**思路**：我们可以构造一个**对称方阵**来解决问题！利用 $A^T A$ 和 $A A^T$。

#### 步骤 1：求解 $V$ 和 $\Sigma$

我们计算 $A^T A$（矩阵 $A$ 的转置乘以 $A$）。将 SVD 公式 $A = U \Sigma V^T$ 代入其中：

$$ A^T A = (U \Sigma V^T)^T (U \Sigma V^T) $$

根据转置的规则 $(AB)^T = B^T A^T$，展开得到：

$$ A^T A = (V \Sigma^T U^T) (U \Sigma V^T) $$

注意中间的 $U^T U$。因为 $U$ 是正交矩阵，所以 **$U^T U = I$ (单位矩阵)**。这消掉了 $U$！

$$ A^T A = V (\Sigma^T \Sigma) V^T $$

因为 $\Sigma$ 是对角阵，$\Sigma^T \Sigma$ 也是对角阵，里面的元素是奇异值 $\sigma$ 的平方（即 $\sigma^2$）。我们令 $\Sigma^2 = \Sigma^T \Sigma$。

$$ A^T A = V \Sigma^2 V^T $$

**这一步非常关键！请仔细看这个式子：**
这不就是**特征值分解**的形式吗？($Matrix = Q \Lambda Q^T$)

**结论 1**：
*   **$V$ (右奇异向量)** 是矩阵 **$A^T A$** 的**特征向量**。
*   **$\Sigma^2$ (奇异值的平方)** 是矩阵 **$A^T A$** 的**特征值**。
*   所以，算出 $A^T A$ 的特征值，开根号，就得到了奇异值 $\sigma$。

#### 步骤 2：求解 $U$

同理，我们计算 $A A^T$：

$$ A A^T = (U \Sigma V^T) (U \Sigma V^T)^T $$
$$ A A^T = (U \Sigma V^T) (V \Sigma^T U^T) $$

这次中间是 $V^T V$。因为 $V$ 是正交矩阵，**$V^T V = I$**。消掉 $V$！

$$ A A^T = U (\Sigma \Sigma^T) U^T $$
$$ A A^T = U \Sigma^2 U^T $$

**结论 2**：
*   **$U$ (左奇异向量)** 是矩阵 **$A A^T$** 的**特征向量**。

---

### 第五部分：举个极简例子演示

假设矩阵 $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{bmatrix}$ (3行2列)。

**1. 求 $V$ 和 $\Sigma$：**
计算 $A^T A = \begin{bmatrix} 1 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$。

这是一个对称矩阵。求它的特征值和特征向量（这一步省去计算过程）：
*   特征值 $\lambda_1 = 3, \lambda_2 = 1$。
*   所以奇异值 $\sigma_1 = \sqrt{3}, \sigma_2 = 1$。
*   特征向量就是 $V$。

**2. 求 $U$：**
计算 $A A^T$ (这是一个 $3 \times 3$ 矩阵)，求它的特征向量，得到 $U$。

最后组合起来就是 $A = U \Sigma V^T$。

---

### 第六部分：SVD 有什么用？（为什么它重要）

既然 $A$ 已经被拆解了，最神奇的事情发生了：

**我们可以丢弃“不重要”的数据。**

在 $\Sigma$ 矩阵中，奇异值是从大到小排列的。
*   $\sigma_1$ 代表信息量最大的方向（拉伸最长）。
*   $\sigma_{100}$ 可能非常小，接近于 0。

如果我们把小的奇异值直接置为 0，再乘回去（$U \times \Sigma_{new} \times V^T$），我们就得到了一个新的矩阵 $A'$。
这个 $A'$ 和原矩阵 $A$ 非常像，但它占用的空间小得多，噪音也少得多。

**应用场景**：
1.  **图片压缩**：保留前 10% 的奇异值，就能还原出肉眼几乎看不出区别的图片。
2.  **降维 (PCA)**：SVD 是主成分分析（PCA）的计算核心，用于把高维数据压扁到低维。
3.  **推荐系统**：Netflix 或 抖音 猜你喜欢什么，底层经常用到 SVD 来分解“用户-商品”矩阵，找出潜在的兴趣特征。

### 总结

SVD 的本质流程：
1.  利用 **$A^T A$** 是对称矩阵的性质。
2.  求出 $A^T A$ 的特征向量，这就是 $V$（旋转方向）。
3.  求出特征值并开根号，这就是 $\sigma$（拉伸程度）。
4.  利用 $A A^T$ 求出 $U$（最终旋转方向）。

