#  中文注释版

为了解MOE的实现方式，以及作用原理，本代码基于 [davidmrau/mixture-of-experts](https://github.com/davidmrau/mixture-of-experts) 添加了中文注释和维度说明，来方便小白更好理解和学习代码
## 引用

如果使用本代码，请引用原作者：

```bibtex
@misc{rau2019moe,
    title={Sparsely-gated Mixture-of-Experts PyTorch implementation},
    author={Rau, David},
    journal={https://github.com/davidmrau/mixture-of-experts},
    year={2019}
}